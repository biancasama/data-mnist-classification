{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, *Yann Le Cun* used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, the CNN predicts what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**] üìö(http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc749772910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_train[0], cmap= 'gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_train[1], cmap= 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: As a first preprocessing step, please normalize your data.** ‚ùì\n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train= X_train/255\n",
    "display(X_train)\n",
    "X_test= X_test/255\n",
    "display(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is the shape of your images** ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ You see that you have 60,000 training images, all of size $(28, 28)$. However...\n",
    "\n",
    "‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`** ‚ùóÔ∏è\n",
    "\n",
    "This last dimension is clearly missing here... Can you guess the reason why?\n",
    "\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ 28 \\times 28 $ pictures are black-and-white. $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1)\n",
    "        \n",
    "* Theoretically, you don't need to know the number of channels unlike colored pictures using for example:\n",
    "    - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "    - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span> </b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **expand_dims** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test` which should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train= np.expand_dims(X_train, axis=3)\n",
    "display(X_train.shape)\n",
    "X_test= np.expand_dims(X_test, axis=3)\n",
    "display(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do to prepare your data is to convert your labels to \"*one-hot encode*\" the categories.\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat= to_categorical(y_train)\n",
    "y_test_cat= to_categorical(y_test)\n",
    "display(y_train_cat)\n",
    "display(y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat.shape, y_test_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` \n",
    "* with the `adam` optimizer\n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def initialize_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, kernel_size=(4, 4), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1 = 8 * (4*4) + 6\n",
    "layer_2 = 16 * 6 * (3*3) + 4 \n",
    "\n",
    "layer_1 + layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2625/2625 [==============================] - 19s 7ms/step - loss: 0.3671 - accuracy: 0.8853 - val_loss: 0.1408 - val_accuracy: 0.9583\n",
      "Epoch 2/5\n",
      "2625/2625 [==============================] - 18s 7ms/step - loss: 0.1138 - accuracy: 0.9659 - val_loss: 0.1009 - val_accuracy: 0.9699\n",
      "Epoch 3/5\n",
      "2625/2625 [==============================] - 19s 7ms/step - loss: 0.0764 - accuracy: 0.9767 - val_loss: 0.0768 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "2625/2625 [==============================] - 19s 7ms/step - loss: 0.0622 - accuracy: 0.9805 - val_loss: 0.0801 - val_accuracy: 0.9777\n",
      "Epoch 5/5\n",
      "2625/2625 [==============================] - 22s 8ms/step - loss: 0.0521 - accuracy: 0.9837 - val_loss: 0.0688 - val_accuracy: 0.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc74151ac40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=30, restore_best_weights=True)\n",
    "model = initialize_model()\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          batch_size=16, \n",
    "          epochs=5,   \n",
    "          validation_split=0.3,\n",
    "          callbacks=[es],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer:</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_cat_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18046/949321468.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_cat_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_test, y_cat_test, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
